# k-近邻算法（KNN)
# 原理
已知一类有标签的数据
1.数据处理（归一化等）（e.g:三个特征 [1 10 100] 考虑三个特征是否同等重要）
2.计算未知标签数据与他们的距离
3.选择距离最近的前k个数据。
4.选择k个标签中出现次数最多的标签作为其标签。

优点：精度高、对异常值不敏感、无数据输入假定。
缺点：计算复杂度高、空间复杂度高。
适用数据范围：数值型和标称型。


# 决策树
信息熵 -sum(p(xi)log2(p(xi)))
# 原理
划分数据集:(按照每个特征划分数据集，计算每个划分的信息熵，选择信息熵最小的划分。) # 划分数据集的大原则是：将无序的数据变得更加有序。
# 伪码
创建分支的伪代码函数createBranch()如下所示：
检测数据集中的每个子项是否属于同一分类：
If so return 类标签；
Else
 寻找划分数据集的最好特征
 划分数据集
 创建分支节点
 for 每个划分的子集
 调用函数createBranch并增加返回结果到分支节点中
 return 分支节点
# 注意
如果数据集已经处理了所有属性，但是类标签依然不是唯一的，此时我们需要决定如何定义该叶子节点，在这种情况下，我们通常会采用多数表决的方法决定该叶子节点的分类。

优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。
缺点：可能会产生过度匹配问题。
适用数据类型：数值型和标称型。


# 朴素贝叶斯
# 贝叶斯决策理论的核心思想：选择具有最高概率的决策。

优点：在数据较少的情况下仍然有效，可以处理多类别问题。
缺点：对于输入数据的准备方式较为敏感。
适用数据类型：标称型数据。

朴素贝叶斯模型由两种类型的概率组成：
1、每个类别的概率P(Cj)；
2、每个特征的条件概率P(Ai|Cj)。（例已知它是鱼它有脚的概率）
公式P(Cj|A)=/求积i(sumj(P(Cj)*P(Ai|Cj)))
# 原理：
1.利用数据计算P(Cj)和P(Ai|Cj)
2.计算P(Cj|A)
3.选出概率最大的分类


# 线性回归
用于拟合线性数据
公式：YY=WX+b
损失函数：sum((YY-Y)^2)/2m


# 逻辑回归
logistic回归是一种广义线性回归，常用于二分类。
式：YY=sigmoid(WX+b))
交叉熵损失函数： -sum(YlnYY+(1-y)ln(1-YY))/m

# softmox回归：
(多分类)(logistic的推广）
激活函数:exp(Z)/sum(exp(Z)) Z=WX+b （多神经元输出）
损失函数：-1/m*sum(sum(y*ln(yy)))


# 支持向量机
# SVM学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。


# Adaboost
# Adaboost是一种迭代算法，其核心思想是针对同一个训练集训练不同的分类器(弱分类器)，然后把这些弱分类器集合起来，构成一个更强的最终分类器（强分类器）。
# 流程
1. 先通过对N个训练样本的学习得到第一个弱分类器；
2. 将分错的样本和其他的新数据一起构成一个新的N个的训练样本，通过对这个样本的学习得到第二个弱分类器 ；
3. 将1和2都分错了的样本加上其他的新样本构成另一个新的N个的训练样本，通过对这个样本的学习得到第三个弱分类器；
4. 最终经过提升的强分类器。即某个数据被分为哪一类要由各分类器权值决定。

基尼系数（Gini）：基尼系数越接近0表明收入分配越是趋向平等


# 树回归
https://zhuanlan.zhihu.com/p/32003259
# 步骤
1）树的生成：基于训练数据集生成决策树，生成的决策树要尽量大；（基尼指数）
2）树的剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时损失函数最小作为剪枝的标准

# 聚类
https://zhuanlan.zhihu.com/p/78382376
# k-means
1.随机确定k个初始点u1, u2...uk作为聚类质心
重复以下过程直到收敛：
    1.对于每一个样例，找到离它最近的质心作为label
    2.对于每一个类j, 更新其质心
终止条件：
1)没有（或最小数目）对象被重新分配给不同的聚类。
2)没有（或最小数目）聚类中心再发生变化。
3)误差平方和局部最小。


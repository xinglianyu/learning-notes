# k-近邻算法（KNN)
# 原理
已知一类有标签的数据
1.数据处理（归一化等）（e.g:三个特征 [1 10 100] 考虑三个特征是否同等重要）
2.计算未知标签数据与他们的距离
3.选择距离最近的前k个数据。
4.选择k个标签中出现次数最多的标签作为其标签。

优点：精度高、对异常值不敏感、无数据输入假定。
缺点：计算复杂度高、空间复杂度高。
适用数据范围：数值型和标称型。



# 决策树
# 原理
1.划分数据集。(按照每个特征划分数据集，计算每个划分的信息熵，选择信息熵最小的划分。) # 划分数据集的大原则是：将无序的数据变得更加有序。
2.
# 伪码
创建分支的伪代码函数createBranch()如下所示：
检测数据集中的每个子项是否属于同一分类：
If so return 类标签；
Else
 寻找划分数据集的最好特征
 划分数据集
 创建分支节点
 for 每个划分的子集
 调用函数createBranch并增加返回结果到分支节点中
 return 分支节点
 
优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。
缺点：可能会产生过度匹配问题。
适用数据类型：数值型和标称型。

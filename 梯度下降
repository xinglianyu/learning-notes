批量梯度下降法（BGD）
随机梯度下降法（SGD）
小批量梯度下降法（MBGD）：把数据集分为多个小的数据集，把每一个小的数据集作为一个独立的个体来更新参数。



加速梯度下降法：

指数加权平均：Vt=beta*Vt-1+(1-beta)*datas.t

动量梯度下降法（Momentum）：对梯度进行指数加权平均

UPW=beta*UPW+(1-beta)*dW  （beta通常取0.9）

RMsprop（均方根传递）算法：（此时可以使用一个较大的学习率）
UP=beta*UP+(1-beta)*(dW**2)   (每个元素的平方）
UPW=dW/sqrt(UP)   （为避免分母为0，可以加上一个比较小的数）

Adam算法

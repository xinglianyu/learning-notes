批量梯度下降法（BGD）
随机梯度下降法（SGD）
小批量梯度下降法（MBGD）：把数据集分为多个小的数据集，把每一个小的数据集作为一个独立的个体来更新参数。



加速梯度下降法：

指数加权平均：Vt=beta*Vt-1+(1-beta)*datas.t

动量梯度下降法（Momentum）：对梯度进行指数加权平均
UPW1=beta1*UPW+(1-beta1)*dW  （beta通常取0.9）

RMsprop（均方根传递）算法：（此时可以使用一个较大的学习率）(0.999)
UP=beta2*UP+(1-beta2)*(dW**2)   (每个元素的平方）
UPW=dW/sqrt(UP)   （为避免分母为0，可以加上一个比较小的数）

Adam算法：结合momentum和RMsprop(UPW1和UP需要修正即除以（1-beta^t) )
UPW=UPW1/sqrt(UP+10^-8)

学习率衰减：
1. 1/（1+beta*study)
2.0.95^(n)*study
3.beta/(sqrt(n))*study
4.分段

